{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOXq7/jA6yosBv9VKWqkyot"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install tiktoken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C6mePKP9PlTl","executionInfo":{"status":"ok","timestamp":1720289312099,"user_tz":-60,"elapsed":8073,"user":{"displayName":"anime fun","userId":"04432068908974540019"}},"outputId":"c36c1f9e-4bea-4729-c247-4457e1874cf6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n"]}]},{"cell_type":"code","source":["import re\n","\n","with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n","   raw_text = f.read()\n","print(\"Total number of character:\", len(raw_text))\n","print(raw_text[:99])\n","preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n","preprocessed = [item.strip() for item in preprocessed if item.strip()]\n","print(len(preprocessed))\n","\"\"\"\n","6 We build a vocabulary by tokenizing the entire text in a training dataset into\n","individual tokens. These individual tokens are then sorted alphabetically, and duplicate tokens\n","are removed. The unique tokens are then aggregated into a vocabulary that defines a mapping\n","from each unique token to a unique integer value. The depicted vocabulary is purposefully small\n","for illustration purposes and contains no punctuation or special characters for simplicity.\n","\"\"\"\n","all_words = sorted(list(set(preprocessed)))\n","vocab_size = len(all_words)\n","print(vocab_size)\n","vocab = {token:integer for integer,token in enumerate(all_words)}\n","for i, item in enumerate(vocab.items()):\n","   print(item)\n","   if i > 50:\n","     break\n","\n","class SimpleTokenizerV1:\n","    def __init__(self, vocab):\n","        self.str_to_int = vocab\n","        self.int_to_str = {i:s for s,i in vocab.items()}\n","\n","    def encode(self, text):\n","        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n","        preprocessed = [\n","            item.strip() for item in preprocessed if item.strip()\n","        ]\n","        ids = [self.str_to_int[s] for s in preprocessed]\n","        return ids\n","\n","    def decode(self, ids):\n","        text = \" \".join([self.int_to_str[i] for i in ids])\n","        # Replace spaces before the specified punctuations\n","        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n","        return text\n","\n","tokenizer = SimpleTokenizerV1(vocab)\n","\n","text = \"\"\"\"It's the last he painted, you know,\"\n","           Mrs. Gisburn said with pardonable pride.\"\"\"\n","ids = tokenizer.encode(text)\n","print(ids)\n","tokenizer.decode(ids)\n","tokenizer.decode(tokenizer.encode(text))\n","all_tokens = sorted(list(set(preprocessed)))\n","all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n","\n","vocab = {token:integer for integer,token in enumerate(all_tokens)}\n","print(len(vocab.items()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B1NDa_YzBiOl","executionInfo":{"status":"ok","timestamp":1720289312100,"user_tz":-60,"elapsed":35,"user":{"displayName":"anime fun","userId":"04432068908974540019"}},"outputId":"558ccbba-63f5-46eb-d51d-7046cbf2362e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of character: 20479\n","I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n","4649\n","1159\n","('!', 0)\n","('\"', 1)\n","(\"'\", 2)\n","('(', 3)\n","(')', 4)\n","(',', 5)\n","('--', 6)\n","('.', 7)\n","(':', 8)\n","(';', 9)\n","('?', 10)\n","('A', 11)\n","('Ah', 12)\n","('Among', 13)\n","('And', 14)\n","('Are', 15)\n","('Arrt', 16)\n","('As', 17)\n","('At', 18)\n","('Be', 19)\n","('Begin', 20)\n","('Burlington', 21)\n","('But', 22)\n","('By', 23)\n","('Carlo', 24)\n","('Carlo;', 25)\n","('Chicago', 26)\n","('Claude', 27)\n","('Come', 28)\n","('Croft', 29)\n","('Destroyed', 30)\n","('Devonshire', 31)\n","('Don', 32)\n","('Dubarry', 33)\n","('Emperors', 34)\n","('Florence', 35)\n","('For', 36)\n","('Gallery', 37)\n","('Gideon', 38)\n","('Gisburn', 39)\n","('Gisburns', 40)\n","('Grafton', 41)\n","('Greek', 42)\n","('Grindle', 43)\n","('Grindle:', 44)\n","('Grindles', 45)\n","('HAD', 46)\n","('Had', 47)\n","('Hang', 48)\n","('Has', 49)\n","('He', 50)\n","('Her', 51)\n","[1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\n","1161\n"]}]},{"cell_type":"code","source":["import importlib\n","import tiktoken\n","\n","print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rEy3GM5uLb-9","executionInfo":{"status":"ok","timestamp":1720289312100,"user_tz":-60,"elapsed":31,"user":{"displayName":"anime fun","userId":"04432068908974540019"}},"outputId":"991c87cf-24e6-4cbf-e6bd-e099ca61a7ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tiktoken version: 0.7.0\n"]}]},{"cell_type":"code","source":["with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n","    raw_text = f.read()\n","\n","enc_text = tokenizer.encode(raw_text)\n","print(len(enc_text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0SDCLpDeLiMW","executionInfo":{"status":"ok","timestamp":1720289312101,"user_tz":-60,"elapsed":26,"user":{"displayName":"anime fun","userId":"04432068908974540019"}},"outputId":"e4c7b0be-956f-41fd-e2ae-a269c9390af8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4649\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DyoqnavrHl0M","executionInfo":{"status":"ok","timestamp":1720289312101,"user_tz":-60,"elapsed":23,"user":{"displayName":"anime fun","userId":"04432068908974540019"}},"outputId":"afbab37e-c203-4b11-a6a3-2ed73edd000d"},"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version: 2.3.0+cu121\n"]}],"source":["import torch\n","print(\"PyTorch version:\", torch.__version__)"]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","\n","\n","class GPTDatasetV1(Dataset):\n","    def __init__(self, txt, tokenizer, max_length, stride):\n","        self.input_ids = []\n","        self.target_ids = []\n","\n","        # Tokenize the entire text\n","        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n","\n","        # Use a sliding window to chunk the book into overlapping sequences of max_length\n","        for i in range(0, len(token_ids) - max_length, stride):\n","            input_chunk = token_ids[i:i + max_length]\n","            target_chunk = token_ids[i + 1: i + max_length + 1]\n","            self.input_ids.append(torch.tensor(input_chunk))\n","            self.target_ids.append(torch.tensor(target_chunk))\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.target_ids[idx]\n","\n"],"metadata":{"id":"37Jb2gJmHmWa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_dataloader_v1(txt, batch_size=4, max_length=256,\n","                         stride=128, shuffle=True, drop_last=True,\n","                         num_workers=0):\n","\n","    # Initialize the tokenizer\n","    tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","    # Create dataset\n","    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n","\n","    # Create dataloader\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=drop_last,\n","        num_workers=num_workers\n","    )\n","\n","    return dataloader"],"metadata":{"id":"-0-9YG67LQYZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n","    raw_text = f.read()\n","dataloader = create_dataloader_v1(\n","    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",")\n","\n","data_iter = iter(dataloader)\n","first_batch = next(data_iter)\n","print(first_batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JQUTVmve9aTY","executionInfo":{"status":"ok","timestamp":1720289312101,"user_tz":-60,"elapsed":19,"user":{"displayName":"anime fun","userId":"04432068908974540019"}},"outputId":"1fe86a95-7968-4c40-8ab9-1a6139189861"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"]}]},{"cell_type":"markdown","source":["The BytePair encoder has a vocabulary size of 50,257:\\"],"metadata":{"id":"UaBaHY0ZJWJN"}},{"cell_type":"code","source":["vocab_size = 50257\n","output_dim = 256\n","\n","token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"],"metadata":{"id":"0LM2SfyRJYff"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["we have a batch size of 8 with 4 tokens each, this results in a 8 x 4 x 256 tensor"],"metadata":{"id":"5FL41k7XJtvb"}},{"cell_type":"code","source":["max_length = 4\n","dataloader = create_dataloader_v1(\n","    raw_text, batch_size=8, max_length=max_length,\n","    stride=max_length, shuffle=False\n",")\n","data_iter = iter(dataloader)\n","inputs, targets = next(data_iter)\n","print(\"Token IDs:\\n\", inputs)\n","print(\"\\nInputs shape:\\n\", inputs.shape)\n","token_embeddings = token_embedding_layer(inputs)\n","print(token_embeddings.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AZglTBgVJkpF","executionInfo":{"status":"ok","timestamp":1720289312102,"user_tz":-60,"elapsed":16,"user":{"displayName":"anime fun","userId":"04432068908974540019"}},"outputId":"69ba5eb0-60ac-4ece-8e3b-8b2bd5d6ea25"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Token IDs:\n"," tensor([[   40,   367,  2885,  1464],\n","        [ 1807,  3619,   402,   271],\n","        [10899,  2138,   257,  7026],\n","        [15632,   438,  2016,   257],\n","        [  922,  5891,  1576,   438],\n","        [  568,   340,   373,   645],\n","        [ 1049,  5975,   284,   502],\n","        [  284,  3285,   326,    11]])\n","\n","Inputs shape:\n"," torch.Size([8, 4])\n","torch.Size([8, 4, 256])\n"]}]},{"cell_type":"markdown","source":["Just like GPT-2 we use absolute position embeddings.so we create another embedding layer"],"metadata":{"id":"EdkvJxjXKGBc"}},{"cell_type":"code","source":["context_length = max_length\n","pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n","pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n","print(pos_embeddings.shape)\n","input_embeddings = token_embeddings + pos_embeddings #creates the input embeddings used in LLM\n","print(input_embeddings.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PpR7MOaYJ16m","executionInfo":{"status":"ok","timestamp":1720289312102,"user_tz":-60,"elapsed":13,"user":{"displayName":"anime fun","userId":"04432068908974540019"}},"outputId":"03e2cb6a-79f9-4e35-e238-0f94c99923b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 256])\n","torch.Size([8, 4, 256])\n"]}]}]}