{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1nwtCSuU2lZYn7Yn-72iX4EAXID3vM0Y7","authorship_tag":"ABX9TyPK21t7ADY3/fKfXIm3mVw6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Pretraining on Unlabeled Data\n"],"metadata":{"id":"X0Zieh9LNq90"}},{"cell_type":"code","source":["!pip install tiktoken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N-cFYvASOe5J","executionInfo":{"status":"ok","timestamp":1722815281502,"user_tz":-60,"elapsed":5578,"user":{"displayName":"anime fun","userId":"04432068908974540019"}},"outputId":"ceb72caa-d659-4b56-ec3c-c5c22efaae94"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n"]}]},{"cell_type":"code","source":["from importlib.metadata import version\n","\n","pkgs = [\"matplotlib\",\n","        \"numpy\",\n","        \"tiktoken\",\n","        \"torch\",\n","        \"tensorflow\" # For OpenAI's pretrained weights\n","       ]\n","for p in pkgs:\n","    print(f\"{p} version: {version(p)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rxpdS9ghNqqA","executionInfo":{"status":"ok","timestamp":1722815281503,"user_tz":-60,"elapsed":13,"user":{"displayName":"anime fun","userId":"04432068908974540019"}},"outputId":"1c963865-b99a-4626-fd5a-d14a2d84b365"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["matplotlib version: 3.7.1\n","numpy version: 1.26.4\n","tiktoken version: 0.7.0\n","torch version: 2.3.1+cu121\n","tensorflow version: 2.17.0\n"]}]},{"cell_type":"markdown","source":["initialize a GPT model"],"metadata":{"id":"zk32otMJN1v1"}},{"cell_type":"code","source":["# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n","# Source for \"Build a Large Language Model From Scratch\"\n","#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n","# Code: https://github.com/rasbt/LLMs-from-scratch\n","#\n","# This file collects all the relevant code that we covered thus far\n","# throughout Chapters 2-4.\n","# This file can be run as a standalone script.\n","\n","import tiktoken\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","#####################################\n","# Chapter 2\n","#####################################\n","\n","\n","class GPTDatasetV1(Dataset):\n","    def __init__(self, txt, tokenizer, max_length, stride):\n","        self.input_ids = []\n","        self.target_ids = []\n","\n","        # Tokenize the entire text\n","        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n","\n","        # Use a sliding window to chunk the book into overlapping sequences of max_length\n","        for i in range(0, len(token_ids) - max_length, stride):\n","            input_chunk = token_ids[i:i + max_length]\n","            target_chunk = token_ids[i + 1: i + max_length + 1]\n","            self.input_ids.append(torch.tensor(input_chunk))\n","            self.target_ids.append(torch.tensor(target_chunk))\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.target_ids[idx]\n","\n","\n","def create_dataloader_v1(txt, batch_size=4, max_length=256,\n","                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n","    # Initialize the tokenizer\n","    tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","    # Create dataset\n","    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n","\n","    # Create dataloader\n","    dataloader = DataLoader(\n","        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n","\n","    return dataloader\n","\n","\n","#####################################\n","# Chapter 3\n","#####################################\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","        super().__init__()\n","        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n","\n","        self.d_out = d_out\n","        self.num_heads = num_heads\n","        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n","\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape\n","\n","        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        # We implicitly split the matrix by adding a `num_heads` dimension\n","        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n","        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n","        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n","\n","        # Original mask truncated to the number of tokens and converted to boolean\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","        # Use the mask to fill attention scores\n","        attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Shape: (b, num_tokens, num_heads, head_dim)\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n","        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n","        context_vec = self.out_proj(context_vec)  # optional projection\n","\n","        return context_vec\n","\n","\n","#####################################\n","# Chapter 4\n","#####################################\n","class LayerNorm(nn.Module):\n","    def __init__(self, emb_dim):\n","        super().__init__()\n","        self.eps = 1e-5\n","        self.scale = nn.Parameter(torch.ones(emb_dim))\n","        self.shift = nn.Parameter(torch.zeros(emb_dim))\n","\n","    def forward(self, x):\n","        mean = x.mean(dim=-1, keepdim=True)\n","        var = x.var(dim=-1, keepdim=True, unbiased=False)\n","        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n","        return self.scale * norm_x + self.shift\n","\n","\n","class GELU(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(\n","            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n","            (x + 0.044715 * torch.pow(x, 3))\n","        ))\n","\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n","            GELU(),\n","            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)\n","\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.att = MultiHeadAttention(\n","            d_in=cfg[\"emb_dim\"],\n","            d_out=cfg[\"emb_dim\"],\n","            context_length=cfg[\"context_length\"],\n","            num_heads=cfg[\"n_heads\"],\n","            dropout=cfg[\"drop_rate\"],\n","            qkv_bias=cfg[\"qkv_bias\"])\n","        self.ff = FeedForward(cfg)\n","        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n","        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n","        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n","\n","    def forward(self, x):\n","        # Shortcut connection for attention block\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        # Shortcut connection for feed-forward block\n","        shortcut = x\n","        x = self.norm2(x)\n","        x = self.ff(x)\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut  # Add the original input back\n","\n","        return x\n","\n","\n","class GPTModel(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        self.trf_blocks = nn.Sequential(\n","            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n","\n","        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n","        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","        tok_embeds = self.tok_emb(in_idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_emb(x)\n","        x = self.trf_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_head(x)\n","        return logits\n","\n","\n","def generate_text_simple(model, idx, max_new_tokens, context_size):\n","    # idx is (B, T) array of indices in the current context\n","    for _ in range(max_new_tokens):\n","\n","        # Crop current context if it exceeds the supported context size\n","        # E.g., if LLM supports only 5 tokens, and the context size is 10\n","        # then only the last 5 tokens are used as context\n","        idx_cond = idx[:, -context_size:]\n","\n","        # Get the predictions\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","\n","        # Focus only on the last time step\n","        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n","        logits = logits[:, -1, :]\n","\n","        # Get the idx of the vocab entry with the highest logits value\n","        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n","\n","        # Append sampled index to the running sequence\n","        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n","\n","    return idx\n","\n","\n","if __name__ == \"__main__\":\n","\n","    GPT_CONFIG_124M = {\n","        \"vocab_size\": 50257,     # Vocabulary size\n","        \"context_length\": 1024,  # Context length\n","        \"emb_dim\": 768,          # Embedding dimension\n","        \"n_heads\": 12,           # Number of attention heads\n","        \"n_layers\": 12,          # Number of layers\n","        \"drop_rate\": 0.1,        # Dropout rate\n","        \"qkv_bias\": False        # Query-Key-Value bias\n","    }\n","\n","    torch.manual_seed(123)\n","    model = GPTModel(GPT_CONFIG_124M)\n","    model.eval()  # disable dropout\n","\n","    start_context = \"Hello, I am\"\n","\n","    tokenizer = tiktoken.get_encoding(\"gpt2\")\n","    encoded = tokenizer.encode(start_context)\n","    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n","\n","    print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n","    print(\"\\nInput text:\", start_context)\n","    print(\"Encoded input text:\", encoded)\n","    print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n","\n","    out = generate_text_simple(\n","        model=model,\n","        idx=encoded_tensor,\n","        max_new_tokens=10,\n","        context_size=GPT_CONFIG_124M[\"context_length\"]\n","    )\n","    decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n","\n","    print(f\"\\n\\n{50*'='}\\n{22*' '}OUT\\n{50*'='}\")\n","    print(\"\\nOutput:\", out)\n","    print(\"Output length:\", len(out[0]))\n","    print(\"Output text:\", decoded_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pkIiSIDVPqx4","executionInfo":{"status":"ok","timestamp":1722815297316,"user_tz":-60,"elapsed":15820,"user":{"displayName":"anime fun","userId":"04432068908974540019"}},"outputId":"76d33c95-3fee-474d-e074-bc176f2d3977"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","==================================================\n","                      IN\n","==================================================\n","\n","Input text: Hello, I am\n","Encoded input text: [15496, 11, 314, 716]\n","encoded_tensor.shape: torch.Size([1, 4])\n","\n","\n","==================================================\n","                      OUT\n","==================================================\n","\n","Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267,\n","         49706, 43231, 47062, 34657]])\n","Output length: 14\n","Output text: Hello, I am Featureiman Byeswickattribute argue logger Normandy Compton analogous\n"]}]},{"cell_type":"code","execution_count":9,"metadata":{"id":"ugnzTo-bKubR","executionInfo":{"status":"ok","timestamp":1722815299194,"user_tz":-60,"elapsed":1914,"user":{"displayName":"anime fun","userId":"04432068908974540019"}}},"outputs":[],"source":["import torch\n","\n","\n","GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,   # Vocabulary size\n","    \"context_length\": 256, # Shortened context length (orig: 1024)\n","    \"emb_dim\": 768,        # Embedding dimension\n","    \"n_heads\": 12,         # Number of attention heads\n","    \"n_layers\": 12,        # Number of layers\n","    \"drop_rate\": 0.1,      # Dropout rate\n","    \"qkv_bias\": False      # Query-key-value bias\n","}\n","\n","torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","model.eval();  # Disable dropout during inference"]},{"cell_type":"code","source":["import tiktoken\n","\n","\n","def text_to_token_ids(text, tokenizer):\n","    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n","    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n","    return encoded_tensor\n","\n","def token_ids_to_text(token_ids, tokenizer):\n","    flat = token_ids.squeeze(0) # remove batch dimension\n","    return tokenizer.decode(flat.tolist())\n","\n","start_context = \"Every effort moves you\"\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","token_ids = generate_text_simple(\n","    model=model,\n","    idx=text_to_token_ids(start_context, tokenizer),\n","    max_new_tokens=10,\n","    context_size=GPT_CONFIG_124M[\"context_length\"]\n",")\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aj3HR7akODci","executionInfo":{"status":"ok","timestamp":1722815301236,"user_tz":-60,"elapsed":2047,"user":{"displayName":"anime fun","userId":"04432068908974540019"}},"outputId":"c7840b51-ecd0-419e-d889-920208139675"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"]}]},{"cell_type":"code","source":["inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n","                       [40,    1107, 588]])   #  \"I really like\"]\n","\n","targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n","                        [1107,  588, 11311]]) #  \" really like chocolate\"]\n","with torch.no_grad():\n","    logits = model(inputs)\n","\n","probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n","print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)\n","print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n","print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n","text_idx = 0\n","target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n","print(\"Text 1:\", target_probas_1)\n","\n","text_idx = 1\n","target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n","print(\"Text 2:\", target_probas_2)\n","# Compute logarithm of all token probabilities\n","log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n","print(log_probas)\n","# Calculate the average probability for each token\n","avg_log_probas = torch.mean(log_probas)\n","print(avg_log_probas)\n","neg_avg_log_probas = avg_log_probas * -1\n","print(neg_avg_log_probas)\n","# Logits have shape (batch_size, num_tokens, vocab_size)\n","print(\"Logits shape:\", logits.shape)\n","\n","# Targets have shape (batch_size, num_tokens)\n","print(\"Targets shape:\", targets.shape)\n","logits_flat = logits.flatten(0, 1)\n","targets_flat = targets.flatten()\n","\n","print(\"Flattened logits:\", logits_flat.shape)\n","print(\"Flattened targets:\", targets_flat.shape)\n","loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n","print(loss)\n","perplexity = torch.exp(loss)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FOK_JJ5-OF82","executionInfo":{"status":"ok","timestamp":1722815301237,"user_tz":-60,"elapsed":13,"user":{"displayName":"anime fun","userId":"04432068908974540019"}},"outputId":"b495c06c-a93f-4038-f325-b0351faf8b46"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 3, 50257])\n","Targets batch 1:  effort moves you\n","Outputs batch 1: Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n","Text 1: tensor([7.4540e-05, 3.1061e-05, 1.1563e-05])\n","Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n","tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n","tensor(-10.7940)\n","tensor(10.7940)\n","Logits shape: torch.Size([2, 3, 50257])\n","Targets shape: torch.Size([2, 3])\n","Flattened logits: torch.Size([6, 50257])\n","Flattened targets: torch.Size([6])\n","tensor(10.7940)\n"]}]},{"cell_type":"markdown","source":["Calculating the training and validation set losses"],"metadata":{"id":"eQnocZnygw7U"}},{"cell_type":"code","source":["import os\n","import urllib.request\n","\n","file_path = \"the-verdict.txt\"\n","url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n","\n","if not os.path.exists(file_path):\n","    with urllib.request.urlopen(url) as response:\n","        text_data = response.read().decode('utf-8')\n","    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n","        file.write(text_data)\n","else:\n","    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","        text_data = file.read()\n","        # First 100 characters\n","print(text_data[:99])\n","total_characters = len(text_data)\n","total_tokens = len(tokenizer.encode(text_data))\n","\n","print(\"Characters:\", total_characters)\n","print(\"Tokens:\", total_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y4VccDaPgy1J","executionInfo":{"status":"ok","timestamp":1722815301721,"user_tz":-60,"elapsed":493,"user":{"displayName":"anime fun","userId":"04432068908974540019"}},"outputId":"c58e3c26-c35d-43c0-e000-fdffb86f8e5b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n","Characters: 20479\n","Tokens: 5145\n"]}]},{"cell_type":"code","source":["# Train/validation ratio\n","train_ratio = 0.90\n","split_idx = int(train_ratio * len(text_data))\n","train_data = text_data[:split_idx]\n","val_data = text_data[split_idx:]\n","\n","\n","torch.manual_seed(123)\n","\n","train_loader = create_dataloader_v1(\n","    train_data,\n","    batch_size=2,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    drop_last=True,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_loader = create_dataloader_v1(\n","    val_data,\n","    batch_size=2,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    drop_last=False,\n","    shuffle=False,\n","    num_workers=0\n",")"],"metadata":{"id":"AJnD9xI7lMHv","executionInfo":{"status":"ok","timestamp":1722815301722,"user_tz":-60,"elapsed":6,"user":{"displayName":"anime fun","userId":"04432068908974540019"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Sanity check\n","\n","if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n","    print(\"Not enough tokens for the training loader. \"\n","          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n","          \"increase the `training_ratio`\")\n","\n","if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n","    print(\"Not enough tokens for the validation loader. \"\n","          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n","          \"decrease the `training_ratio`\")"],"metadata":{"id":"w-XzFgeWlP_l","executionInfo":{"status":"ok","timestamp":1722815301722,"user_tz":-60,"elapsed":5,"user":{"displayName":"anime fun","userId":"04432068908974540019"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def calc_loss_batch(input_batch, target_batch, model, device):\n","    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n","    logits = model(input_batch)\n","    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n","    return loss\n","\n","\n","def calc_loss_loader(data_loader, model, device, num_batches=None):\n","    total_loss = 0.\n","    if len(data_loader) == 0:\n","        return float(\"nan\")\n","    elif num_batches is None:\n","        num_batches = len(data_loader)\n","    else:\n","        # Reduce the number of batches to match the total number of batches in the data loader\n","        # if num_batches exceeds the number of batches in the data loader\n","        num_batches = min(num_batches, len(data_loader))\n","    for i, (input_batch, target_batch) in enumerate(data_loader):\n","        if i < num_batches:\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            total_loss += loss.item()\n","        else:\n","            break\n","    return total_loss / num_batches"],"metadata":{"id":"-sjTSUdJlStI","executionInfo":{"status":"ok","timestamp":1722815301722,"user_tz":-60,"elapsed":4,"user":{"displayName":"anime fun","userId":"04432068908974540019"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n","\n","\n","torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n","\n","with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n","    train_loss = calc_loss_loader(train_loader, model, device)\n","    val_loss = calc_loss_loader(val_loader, model, device)\n","\n","print(\"Training loss:\", train_loss)\n","print(\"Validation loss:\", val_loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jfvouzJZleHc","executionInfo":{"status":"ok","timestamp":1722815340893,"user_tz":-60,"elapsed":39175,"user":{"displayName":"anime fun","userId":"04432068908974540019"}},"outputId":"f30fc9f1-9fe3-4588-bf8d-c9dd9c3fee7a"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Training loss: 10.98758347829183\n","Validation loss: 10.98110580444336\n"]}]},{"cell_type":"markdown","source":["LLM Training"],"metadata":{"id":"tbQPAR53lki6"}},{"cell_type":"code","source":["def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n","                       eval_freq, eval_iter, start_context, tokenizer):\n","    # Initialize lists to track losses and tokens seen\n","    train_losses, val_losses, track_tokens_seen = [], [], []\n","    tokens_seen, global_step = 0, -1\n","\n","    # Main training loop\n","    for epoch in range(num_epochs):\n","        model.train()  # Set model to training mode\n","\n","        for input_batch, target_batch in train_loader:\n","            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            loss.backward() # Calculate loss gradients\n","            optimizer.step() # Update model weights using loss gradients\n","            tokens_seen += input_batch.numel()\n","            global_step += 1\n","\n","            # Optional evaluation step\n","            if global_step % eval_freq == 0:\n","                train_loss, val_loss = evaluate_model(\n","                    model, train_loader, val_loader, device, eval_iter)\n","                train_losses.append(train_loss)\n","                val_losses.append(val_loss)\n","                track_tokens_seen.append(tokens_seen)\n","                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n","                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n","\n","        # Print a sample text after each epoch\n","        generate_and_print_sample(\n","            model, tokenizer, device, start_context\n","        )\n","\n","    return train_losses, val_losses, track_tokens_seen\n","\n","\n","def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n","    model.eval()\n","    with torch.no_grad():\n","        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n","        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n","    model.train()\n","    return train_loss, val_loss\n","\n","\n","def generate_and_print_sample(model, tokenizer, device, start_context):\n","    model.eval()\n","    context_size = model.pos_emb.weight.shape[0]\n","    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n","    with torch.no_grad():\n","        token_ids = generate_text_simple(\n","            model=model, idx=encoded,\n","            max_new_tokens=50, context_size=context_size\n","        )\n","        decoded_text = token_ids_to_text(token_ids, tokenizer)\n","        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n","    model.train()"],"metadata":{"id":"YqjG2zsnlp76","executionInfo":{"status":"ok","timestamp":1722815340894,"user_tz":-60,"elapsed":25,"user":{"displayName":"anime fun","userId":"04432068908974540019"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n","\n","num_epochs = 10\n","train_losses, val_losses, tokens_seen = train_model_simple(\n","    model, train_loader, val_loader, optimizer, device,\n","    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n","    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"66we8Qx6lyvS","executionInfo":{"status":"ok","timestamp":1722816678833,"user_tz":-60,"elapsed":1337963,"user":{"displayName":"anime fun","userId":"04432068908974540019"}},"outputId":"7955fa1d-3784-4b49-c156-5572d4f661ee"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Ep 1 (Step 000000): Train loss 9.783, Val loss 9.927\n","Ep 1 (Step 000005): Train loss 7.985, Val loss 8.335\n","Every effort moves you,,,,,,,,,,,,.                                     \n","Ep 2 (Step 000010): Train loss 6.753, Val loss 7.048\n","Ep 2 (Step 000015): Train loss 6.114, Val loss 6.573\n","Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,,,,, and,,,, and,, and,,,,, and,,,,,,\n","Ep 3 (Step 000020): Train loss 5.525, Val loss 6.490\n","Ep 3 (Step 000025): Train loss 5.324, Val loss 6.387\n","Every effort moves you, and to the picture.                      \"I, and the of the of the's the honour, and, and I had been, and I\n","Ep 4 (Step 000030): Train loss 4.761, Val loss 6.360\n","Ep 4 (Step 000035): Train loss 4.461, Val loss 6.258\n","Every effort moves you of the to the picture--as of the picture--as I had been \" it was his \" I was the     \"I was his I had been the his pictures--and it the picture and I had been the picture of\n","Ep 5 (Step 000040): Train loss 3.833, Val loss 6.196\n","Every effort moves you know the \"Oh, and he was not the fact by his last word.         \"I was.      \"Oh, I felt a little a little the    \n","Ep 6 (Step 000045): Train loss 3.352, Val loss 6.139\n","Ep 6 (Step 000050): Train loss 2.861, Val loss 6.112\n","Every effort moves you know; and my dear, and he was not the fact with a little of the house of the fact of the fact, and.                       \n","Ep 7 (Step 000055): Train loss 2.347, Val loss 6.138\n","Ep 7 (Step 000060): Train loss 2.084, Val loss 6.179\n","Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I looked--as of the fact, and I felt him--his back his head to the donkey. \"Oh, and_--because he had always _\n","Ep 8 (Step 000065): Train loss 1.521, Val loss 6.176\n","Ep 8 (Step 000070): Train loss 1.272, Val loss 6.178\n","Every effort moves you?\" \"I didn't bear the picture--I told me.  \"I looked up, and went on groping and Mrs. I was back the head to look up at the honour being _mine_--because he was when I\n","Ep 9 (Step 000075): Train loss 1.000, Val loss 6.277\n","Ep 9 (Step 000080): Train loss 0.718, Val loss 6.281\n","Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n","Ep 10 (Step 000085): Train loss 0.506, Val loss 6.325\n","Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to the donkey again. I saw that, and down the room, when I\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"2_4bMhzwlplQ"}},{"cell_type":"code","source":["model.to(\"cpu\")\n","model.eval()\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","token_ids = generate_text_simple(\n","    model=model,\n","    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n","    max_new_tokens=25,\n","    context_size=GPT_CONFIG_124M[\"context_length\"]\n",")\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y6awXYCSmXl7","executionInfo":{"status":"ok","timestamp":1722816682951,"user_tz":-60,"elapsed":4136,"user":{"displayName":"anime fun","userId":"04432068908974540019"}},"outputId":"5dd2494e-cd53-4e12-8d5c-ca59e657f8c2"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," Every effort moves you?\"\n","\n","\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n","\n","\n"]}]},{"cell_type":"code","source":["def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n","\n","    # For-loop is the same as before: Get logits, and only focus on last time step\n","    for _ in range(max_new_tokens):\n","        idx_cond = idx[:, -context_size:]\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","        logits = logits[:, -1, :]\n","\n","        # New: Filter logits with top_k sampling\n","        if top_k is not None:\n","            # Keep only top_k values\n","            top_logits, _ = torch.topk(logits, top_k)\n","            min_val = top_logits[:, -1]\n","            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n","\n","        # New: Apply temperature scaling\n","        if temperature > 0.0:\n","            logits = logits / temperature\n","\n","            # Apply softmax to get probabilities\n","            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n","\n","            # Sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n","\n","        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n","        else:\n","            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n","\n","        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n","            break\n","\n","        # Same as before: append sampled index to the running sequence\n","        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n","\n","    return idx\n"],"metadata":{"id":"s1_FInh0mcAL","executionInfo":{"status":"ok","timestamp":1722816682951,"user_tz":-60,"elapsed":25,"user":{"displayName":"anime fun","userId":"04432068908974540019"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","\n","token_ids = generate(\n","    model=model,\n","    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n","    max_new_tokens=15,\n","    context_size=GPT_CONFIG_124M[\"context_length\"],\n","    top_k=25,\n","    temperature=1.4\n",")\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a30ufRw0mvPg","executionInfo":{"status":"ok","timestamp":1722816685115,"user_tz":-60,"elapsed":2188,"user":{"displayName":"anime fun","userId":"04432068908974540019"}},"outputId":"9892dabb-9970-4410-d6e8-805aef12f81b"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," Every effort moves you know began to my surprise, a little it was the\n","\"Ah enough\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"model.pth\")\n","model = GPTModel(GPT_CONFIG_124M)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.load_state_dict(torch.load(\"model.pth\", map_location=device, weights_only=True))\n","model.eval();\n","torch.save({\n","    \"model_state_dict\": model.state_dict(),\n","    \"optimizer_state_dict\": optimizer.state_dict(),\n","    },\n","    \"model_and_optimizer.pth\"\n",")\n","\n","checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n","\n","model = GPTModel(GPT_CONFIG_124M)\n","model.load_state_dict(checkpoint[\"model_state_dict\"])\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n","optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n","model.train();"],"metadata":{"id":"uDn1NDsNnFBq","executionInfo":{"status":"ok","timestamp":1722816743192,"user_tz":-60,"elapsed":58082,"user":{"displayName":"anime fun","userId":"04432068908974540019"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n","# Source for \"Build a Large Language Model From Scratch\"\n","#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n","# Code: https://github.com/rasbt/LLMs-from-scratch\n","\n","\n","import os\n","import urllib.request\n","\n","# import requests\n","import json\n","import numpy as np\n","import tensorflow as tf\n","from tqdm import tqdm\n","\n","\n","def download_and_load_gpt2(model_size, models_dir):\n","    # Validate model size\n","    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n","    if model_size not in allowed_sizes:\n","        raise ValueError(f\"Model size not in {allowed_sizes}\")\n","\n","    # Define paths\n","    model_dir = os.path.join(models_dir, model_size)\n","    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n","    filenames = [\n","        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n","        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n","        \"model.ckpt.meta\", \"vocab.bpe\"\n","    ]\n","\n","    # Download files\n","    os.makedirs(model_dir, exist_ok=True)\n","    for filename in filenames:\n","        file_url = os.path.join(base_url, model_size, filename)\n","        file_path = os.path.join(model_dir, filename)\n","        download_file(file_url, file_path)\n","\n","    # Load settings and params\n","    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n","    settings = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n","    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n","\n","    return settings, params\n","\n","\n","def download_file(url, destination):\n","    # Send a GET request to download the file\n","\n","    try:\n","        with urllib.request.urlopen(url) as response:\n","            # Get the total file size from headers, defaulting to 0 if not present\n","            file_size = int(response.headers.get(\"Content-Length\", 0))\n","\n","            # Check if file exists and has the same size\n","            if os.path.exists(destination):\n","                file_size_local = os.path.getsize(destination)\n","                if file_size == file_size_local:\n","                    print(f\"File already exists and is up-to-date: {destination}\")\n","                    return\n","\n","            # Define the block size for reading the file\n","            block_size = 1024  # 1 Kilobyte\n","\n","            # Initialize the progress bar with total file size\n","            progress_bar_description = os.path.basename(url)  # Extract filename from URL\n","            with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n","                # Open the destination file in binary write mode\n","                with open(destination, \"wb\") as file:\n","                    # Read the file in chunks and write to destination\n","                    while True:\n","                        chunk = response.read(block_size)\n","                        if not chunk:\n","                            break\n","                        file.write(chunk)\n","                        progress_bar.update(len(chunk))  # Update progress bar\n","    except urllib.error.HTTPError:\n","        s = (\n","            f\"The specified URL ({url}) is incorrect, the internet connection cannot be established,\"\n","            \"\\nor the requested file is temporarily unavailable.\\nPlease visit the following website\"\n","            \" for help: https://github.com/rasbt/LLMs-from-scratch/discussions/273\")\n","        print(s)\n","\n","\n","# Alternative way using `requests`\n","\"\"\"\n","def download_file(url, destination):\n","    # Send a GET request to download the file in streaming mode\n","    response = requests.get(url, stream=True)\n","\n","    # Get the total file size from headers, defaulting to 0 if not present\n","    file_size = int(response.headers.get(\"content-length\", 0))\n","\n","    # Check if file exists and has the same size\n","    if os.path.exists(destination):\n","        file_size_local = os.path.getsize(destination)\n","        if file_size == file_size_local:\n","            print(f\"File already exists and is up-to-date: {destination}\")\n","            return\n","\n","    # Define the block size for reading the file\n","    block_size = 1024  # 1 Kilobyte\n","\n","    # Initialize the progress bar with total file size\n","    progress_bar_description = url.split(\"/\")[-1]  # Extract filename from URL\n","    with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n","        # Open the destination file in binary write mode\n","        with open(destination, \"wb\") as file:\n","            # Iterate over the file data in chunks\n","            for chunk in response.iter_content(block_size):\n","                progress_bar.update(len(chunk))  # Update progress bar\n","                file.write(chunk)  # Write the chunk to the file\n","\"\"\"\n","\n","\n","def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n","    # Initialize parameters dictionary with empty blocks for each layer\n","    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n","\n","    # Iterate over each variable in the checkpoint\n","    for name, _ in tf.train.list_variables(ckpt_path):\n","        # Load the variable and remove singleton dimensions\n","        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n","\n","        # Process the variable name to extract relevant parts\n","        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n","\n","        # Identify the target dictionary for the variable\n","        target_dict = params\n","        if variable_name_parts[0].startswith(\"h\"):\n","            layer_number = int(variable_name_parts[0][1:])\n","            target_dict = params[\"blocks\"][layer_number]\n","\n","        # Recursively access or create nested dictionaries\n","        for key in variable_name_parts[1:-1]:\n","            target_dict = target_dict.setdefault(key, {})\n","\n","        # Assign the variable array to the last key\n","        last_key = variable_name_parts[-1]\n","        target_dict[last_key] = variable_array\n","\n","    return params"],"metadata":{"id":"SoExbwXeuIuU","executionInfo":{"status":"ok","timestamp":1722816756344,"user_tz":-60,"elapsed":13170,"user":{"displayName":"anime fun","userId":"04432068908974540019"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["print(\"TensorFlow version:\", version(\"tensorflow\"))\n","print(\"tqdm version:\", version(\"tqdm\"))\n","settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"utQe5Jm-ngYw","executionInfo":{"status":"ok","timestamp":1722816761511,"user_tz":-60,"elapsed":5200,"user":{"displayName":"anime fun","userId":"04432068908974540019"}},"outputId":"55d2fde9-836c-4f9a-d889-95fa9bd9570a"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version: 2.17.0\n","tqdm version: 4.66.4\n","File already exists and is up-to-date: gpt2/124M/checkpoint\n","File already exists and is up-to-date: gpt2/124M/encoder.json\n","File already exists and is up-to-date: gpt2/124M/hparams.json\n","File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n","File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n","File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n","File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"]}]},{"cell_type":"code","source":["print(\"Parameter dictionary keys:\", params.keys())\n","print(\"Settings:\", settings)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FbOx4tbvuh7_","executionInfo":{"status":"ok","timestamp":1722816761512,"user_tz":-60,"elapsed":54,"user":{"displayName":"anime fun","userId":"04432068908974540019"}},"outputId":"5fe8fd26-1f22-4561-95c7-1a560ce7e64b"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n","Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n"]}]},{"cell_type":"code","source":["# Define model configurations in a dictionary for compactness\n","model_configs = {\n","    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n","    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n","    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n","    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n","}\n","\n","# Copy the base configuration and update with specific model settings\n","model_name = \"gpt2-small (124M)\"  # Example model name\n","NEW_CONFIG = GPT_CONFIG_124M.copy()\n","NEW_CONFIG.update(model_configs[model_name])\n","NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n","\n","gpt = GPTModel(NEW_CONFIG)\n","gpt.eval();"],"metadata":{"id":"QlgUJ2YuuplF","executionInfo":{"status":"ok","timestamp":1722816763830,"user_tz":-60,"elapsed":2366,"user":{"displayName":"anime fun","userId":"04432068908974540019"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["def assign(left, right):\n","    if left.shape != right.shape:\n","        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n","    return torch.nn.Parameter(torch.tensor(right))"],"metadata":{"id":"Yhc0lGPJus5U","executionInfo":{"status":"ok","timestamp":1722816763831,"user_tz":-60,"elapsed":11,"user":{"displayName":"anime fun","userId":"04432068908974540019"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","def load_weights_into_gpt(gpt, params):\n","    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n","    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n","\n","    for b in range(len(params[\"blocks\"])):\n","        q_w, k_w, v_w = np.split(\n","            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n","        gpt.trf_blocks[b].att.W_query.weight = assign(\n","            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n","        gpt.trf_blocks[b].att.W_key.weight = assign(\n","            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n","        gpt.trf_blocks[b].att.W_value.weight = assign(\n","            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n","\n","        q_b, k_b, v_b = np.split(\n","            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n","        gpt.trf_blocks[b].att.W_query.bias = assign(\n","            gpt.trf_blocks[b].att.W_query.bias, q_b)\n","        gpt.trf_blocks[b].att.W_key.bias = assign(\n","            gpt.trf_blocks[b].att.W_key.bias, k_b)\n","        gpt.trf_blocks[b].att.W_value.bias = assign(\n","            gpt.trf_blocks[b].att.W_value.bias, v_b)\n","\n","        gpt.trf_blocks[b].att.out_proj.weight = assign(\n","            gpt.trf_blocks[b].att.out_proj.weight,\n","            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n","        gpt.trf_blocks[b].att.out_proj.bias = assign(\n","            gpt.trf_blocks[b].att.out_proj.bias,\n","            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n","\n","        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n","            gpt.trf_blocks[b].ff.layers[0].weight,\n","            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n","        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n","            gpt.trf_blocks[b].ff.layers[0].bias,\n","            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n","        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n","            gpt.trf_blocks[b].ff.layers[2].weight,\n","            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n","        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n","            gpt.trf_blocks[b].ff.layers[2].bias,\n","            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n","\n","        gpt.trf_blocks[b].norm1.scale = assign(\n","            gpt.trf_blocks[b].norm1.scale,\n","            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n","        gpt.trf_blocks[b].norm1.shift = assign(\n","            gpt.trf_blocks[b].norm1.shift,\n","            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n","        gpt.trf_blocks[b].norm2.scale = assign(\n","            gpt.trf_blocks[b].norm2.scale,\n","            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n","        gpt.trf_blocks[b].norm2.shift = assign(\n","            gpt.trf_blocks[b].norm2.shift,\n","            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n","\n","    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n","    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n","    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n","\n","\n","load_weights_into_gpt(gpt, params)\n","gpt.to(device);"],"metadata":{"id":"0ss-rCflux0c","executionInfo":{"status":"ok","timestamp":1722816764278,"user_tz":-60,"elapsed":455,"user":{"displayName":"anime fun","userId":"04432068908974540019"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","\n","token_ids = generate(\n","    model=gpt,\n","    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n","    max_new_tokens=25,\n","    context_size=NEW_CONFIG[\"context_length\"],\n","    top_k=50,\n","    temperature=1.5\n",")\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5-P6CsmnvOx_","executionInfo":{"status":"ok","timestamp":1722816769621,"user_tz":-60,"elapsed":5350,"user":{"displayName":"anime fun","userId":"04432068908974540019"}},"outputId":"aa9e34b8-ef8c-492d-e5d7-44e585d3df9a"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," Every effort moves you toward an equal share for each vote plus half. Inequality is often not an accurate representation of human worth; to know the\n"]}]}]}